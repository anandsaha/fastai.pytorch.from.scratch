{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mylib.data_loaders as data_loaders\n",
    "import mylib.data_transformers as data_transformers\n",
    "import mylib.datasets as ds\n",
    "import mylib.models_repo as models_repo\n",
    "import mylib.optimizer_repo as optimizer_repo\n",
    "import mylib.scheduler_repo as scheduler_repo\n",
    "import mylib.trainer as trainer\n",
    "import mylib.model_saver as model_saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets.folder import ImageFolder\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/home/as/datasets/kaggle.dog.breed/train\"\n",
    "csv_file = \"/home/as/datasets/kaggle.dog.breed/labels.csv\"\n",
    "sample_submission_file = '/home/as/datasets/kaggle.dog.breed/sample_submission.csv'\n",
    "\n",
    "num_classes = 120\n",
    "img_size  = 224\n",
    "scale_img_size = 300\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm = data_transformers.pytorch_zoo_normaliser\n",
    "trans = data_transformers.get_transformer(img_size, scale_img_size, norm, False)\n",
    "trans_aug = data_transformers.get_transformer(img_size, scale_img_size, norm, True)\n",
    "trans_valid = data_transformers.get_test_valid_transformer(img_size, scale_img_size, norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#np.random.seed(42)\n",
    "labels = pd.read_csv(csv_file)\n",
    "mask = np.random.rand(len(labels)) < 0.75\n",
    "labels_train = labels[mask].values\n",
    "labels_valid = labels[~mask].values\n",
    "sub = pd.read_csv(sample_submission_file,)\n",
    "breeds = list(sub)[1:]\n",
    "\n",
    "train_images = ds.DatasetFromCSV(labels_train, breeds, data_path, 'jpg', transform=trans)\n",
    "train_images_aug = ds.DatasetFromCSV(labels_train, breeds, data_path, 'jpg', transform=trans_aug)\n",
    "valid_images = ds.DatasetFromCSV(labels_valid, breeds, data_path, 'jpg', transform=trans_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(breeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train instances 7652\n",
      "Number of valid instances 2570\n"
     ]
    }
   ],
   "source": [
    "print('Number of train instances', len(train_images))\n",
    "print('Number of valid instances', len(valid_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes ['affenpinscher', 'afghan_hound', 'african_hunting_dog', 'airedale', 'american_staffordshire_terrier', 'appenzeller', 'australian_terrier', 'basenji', 'basset', 'beagle', 'bedlington_terrier', 'bernese_mountain_dog', 'black-and-tan_coonhound', 'blenheim_spaniel', 'bloodhound', 'bluetick', 'border_collie', 'border_terrier', 'borzoi', 'boston_bull', 'bouvier_des_flandres', 'boxer', 'brabancon_griffon', 'briard', 'brittany_spaniel', 'bull_mastiff', 'cairn', 'cardigan', 'chesapeake_bay_retriever', 'chihuahua', 'chow', 'clumber', 'cocker_spaniel', 'collie', 'curly-coated_retriever', 'dandie_dinmont', 'dhole', 'dingo', 'doberman', 'english_foxhound', 'english_setter', 'english_springer', 'entlebucher', 'eskimo_dog', 'flat-coated_retriever', 'french_bulldog', 'german_shepherd', 'german_short-haired_pointer', 'giant_schnauzer', 'golden_retriever', 'gordon_setter', 'great_dane', 'great_pyrenees', 'greater_swiss_mountain_dog', 'groenendael', 'ibizan_hound', 'irish_setter', 'irish_terrier', 'irish_water_spaniel', 'irish_wolfhound', 'italian_greyhound', 'japanese_spaniel', 'keeshond', 'kelpie', 'kerry_blue_terrier', 'komondor', 'kuvasz', 'labrador_retriever', 'lakeland_terrier', 'leonberg', 'lhasa', 'malamute', 'malinois', 'maltese_dog', 'mexican_hairless', 'miniature_pinscher', 'miniature_poodle', 'miniature_schnauzer', 'newfoundland', 'norfolk_terrier', 'norwegian_elkhound', 'norwich_terrier', 'old_english_sheepdog', 'otterhound', 'papillon', 'pekinese', 'pembroke', 'pomeranian', 'pug', 'redbone', 'rhodesian_ridgeback', 'rottweiler', 'saint_bernard', 'saluki', 'samoyed', 'schipperke', 'scotch_terrier', 'scottish_deerhound', 'sealyham_terrier', 'shetland_sheepdog', 'shih-tzu', 'siberian_husky', 'silky_terrier', 'soft-coated_wheaten_terrier', 'staffordshire_bullterrier', 'standard_poodle', 'standard_schnauzer', 'sussex_spaniel', 'tibetan_mastiff', 'tibetan_terrier', 'toy_poodle', 'toy_terrier', 'vizsla', 'walker_hound', 'weimaraner', 'welsh_springer_spaniel', 'west_highland_white_terrier', 'whippet', 'wire-haired_fox_terrier', 'yorkshire_terrier']\n"
     ]
    }
   ],
   "source": [
    "print('Classes', train_images.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = data_loaders.get_data_loader(train_images, batch_size)\n",
    "train_loader_aug = data_loaders.get_data_loader(train_images_aug, batch_size)\n",
    "valid_loader = data_loaders.get_data_loader(valid_images, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = models_repo.resnet_vanilla(num_classes)\n",
    "    criteria, optimizer = optimizer_repo.sgd(model, 0.01)\n",
    "    scheduler = scheduler_repo.step_lr(optimizer)\n",
    "\n",
    "    best_model = trainer.experiment(model, criteria, optimizer, scheduler, train_loader, valid_loader, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    model = models_repo.resnet_vanilla(num_classes)\n",
    "    criteria, optimizer = optimizer_repo.sgd(model, 0.01)\n",
    "    scheduler = scheduler_repo.step_lr(optimizer)\n",
    "\n",
    "    best_model = trainer.experiment(model, criteria, optimizer, scheduler, train_loader_aug, valid_loader, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.conv.Conv2d'> 1 0\n",
      "<class 'torch.nn.modules.batchnorm.BatchNorm2d'> 2 0\n",
      "<class 'torch.nn.modules.activation.ReLU'> 0 0\n",
      "<class 'torch.nn.modules.pooling.MaxPool2d'> 0 0\n",
      "<class 'torch.nn.modules.container.Sequential'> 18 0\n",
      "<class 'torch.nn.modules.container.Sequential'> 27 0\n",
      "<class 'torch.nn.modules.container.Sequential'> 39 0\n",
      "<class 'torch.nn.modules.container.Sequential'> 21 0\n",
      "<class 'mylib.models_repo.AdaptiveConcatPool2d'> 0 0\n",
      "<class 'mylib.models_repo.Flatten'> 0 0\n",
      "<class 'torch.nn.modules.batchnorm.BatchNorm1d'> 2 2\n",
      "<class 'torch.nn.modules.linear.Linear'> 2 2\n",
      "<class 'torch.nn.modules.dropout.Dropout'> 0 0\n",
      "<class 'torch.nn.modules.activation.ReLU'> 0 0\n",
      "<class 'torch.nn.modules.batchnorm.BatchNorm1d'> 2 2\n",
      "<class 'torch.nn.modules.linear.Linear'> 2 2\n",
      "<class 'torch.nn.modules.dropout.Dropout'> 0 0\n",
      "<class 'torch.nn.modules.activation.ReLU'> 0 0\n",
      "<class 'torch.nn.modules.batchnorm.BatchNorm1d'> 2 2\n",
      "<class 'torch.nn.modules.dropout.Dropout'> 0 0\n",
      "<class 'torch.nn.modules.linear.Linear'> 2 2\n",
      "<class 'torch.nn.modules.activation.LogSoftmax'> 0 0\n"
     ]
    }
   ],
   "source": [
    "# model, params_to_optimize = models_repo.inception3_extra_layers_test(num_classes, top_layers_to_freeze=15, debug=True)\n",
    "model, params_to_optimize = models_repo.resnet34_extra_layers(num_classes, top_layers_to_freeze=8, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5efd94028a46f8bd58f7ed033a7f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [00:14,  4.17it/s]\n",
      "21it [00:04,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss  : 0.032192950901783264 , Acc: 0.16348667015159435\n",
      "Validation Loss: 0.023313436526732685 , Acc: 0.5062256809338521\n",
      "############################### Better model found\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "60it [00:14,  4.25it/s]\n",
      "21it [00:04,  5.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss  : 0.020091522818015496 , Acc: 0.4645844223732358\n",
      "Validation Loss: 0.014894601575131547 , Acc: 0.6280155642023346\n",
      "############################### Better model found\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "60it [00:14,  4.27it/s]\n",
      "21it [00:04,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss  : 0.014027793963293225 , Acc: 0.5948771562990068\n",
      "Validation Loss: 0.01070841557784767 , Acc: 0.6976653696498054\n",
      "############################### Better model found\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "60it [00:14,  4.25it/s]\n",
      "21it [00:04,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss  : 0.011106352748761174 , Acc: 0.6583899634082593\n",
      "Validation Loss: 0.00898708471528287 , Acc: 0.724124513618677\n",
      "############################### Better model found\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "60it [00:14,  4.23it/s]\n",
      "21it [00:04,  4.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss  : 0.009516675264749362 , Acc: 0.6944589649764767\n",
      "Validation Loss: 0.008372858178290876 , Acc: 0.7229571984435798\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model, params_to_optimize = models_repo.resnet34_extra_layers(num_classes, top_layers_to_freeze=7)\n",
    "criteria, optimizer = optimizer_repo.sgd(model, 0.01, params_to_optimize=params_to_optimize)\n",
    "scheduler = scheduler_repo.step_lr(optimizer, 20, 0.01)\n",
    "\n",
    "ret = trainer.train(model, criteria, optimizer, scheduler, train_loader_aug, valid_loader, 5)\n",
    "model, train_loss_trend, val_loss_trend, best_epoch, best_accuracy, best_loss = ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_trend)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_loss_trend)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = model_saver.save_checkpoint(best_epoch, 'inception3', best_model, best_accuracy, \n",
    "                                best_loss, optimizer, 'dog.breed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pred(img_path):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img = trans_valid(img)\n",
    "    img = autograd.Variable(torch.unsqueeze(img, 0).cuda())\n",
    "    bm = best_model.cuda()\n",
    "    output = bm(img)\n",
    "    arr = output.data.cpu().numpy()[0]\n",
    "    arr = np.exp(arr) / np.sum(np.exp(arr))\n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('/tmp/submission.csv', 'w')\n",
    "\n",
    "\n",
    "for file in os.listdir('/home/as/datasets/kaggle.dog.breed/test'):\n",
    "    p = os.path.join('/home/as/datasets/kaggle.dog.breed/test', file)\n",
    "    arr = pred(p)\n",
    "    s = ''\n",
    "    for a in arr:\n",
    "        s += ',' + str(a)\n",
    "    f.write(file.replace('.jpg', '') + s + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments from here on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.argmax(arr)\n",
    "np.argmax(output.data.cpu().numpy()[0])\n",
    "breeds[41]\n",
    "output.data.cpu().numpy()[0][59]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = []\n",
    "w = []\n",
    "\n",
    "for file in os.listdir('/home/as/datasets/kaggle.dog.breed/train'):\n",
    "    p = os.path.join('/home/as/datasets/kaggle.dog.breed/train', file)\n",
    "    img = Image.open(p).convert('RGB')\n",
    "    h.append(img.size[0])\n",
    "    w.append(img.size[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(340 - np.min(h))\n",
    "print(340 - np.min(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.randint(0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "def children(m): return m if isinstance(m, (list, tuple)) else list(m.children())\n",
    "\n",
    "def num_features(m):\n",
    "    c=children(m)\n",
    "    if len(c)==0: return None\n",
    "    for l in reversed(c):\n",
    "        if hasattr(l, 'num_features'): return l.num_features\n",
    "        res = num_features(l)\n",
    "        if res is not None: return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = models.inception_v3(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for c in model.children():\n",
    "    print(type(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = models.inception_v3(pretrained=True)\n",
    "modeli = models.inception_v3(pretrained=True)\n",
    "inception3_layers_to_extract = 17\n",
    "\n",
    "layers_list = list(model.children())\n",
    "feature_extracting_layers = layers_list[:inception3_layers_to_extract]\n",
    "\n",
    "feature_extracting_layers += [models_repo.AdaptiveConcatPool2d(), models_repo.Flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
